---
layout: post
title: 理解LSTM
date: 2017-12-29
categories: blog
tags: [NLP]
author: gafei
---

### 参考文章：  
[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458) by hanbingtao  
[(译) 理解 LSTM 网络](https://www.jianshu.com/p/9dc9f41f0b29) by Not_GOD  
[LSTM神经网络输入输出究竟是怎样的？](https://www.zhihu.com/question/41949741) by 知乎  
[（译）理解 LSTM 网络 （Understanding LSTM Networks by colah）](http://blog.csdn.net/Jerr__y/article/details/58598296) by huangyongye

## 在讲理解LSTM之前，首先理解一下RNN和基本的语言模型  

### 语言模型  

RNN是在自然语言处理领域中最先被用起来的，比如，RNN可以为语言模型来建模。  

比如根据上下文推测目标语句，根据现有语境推测上下文，机器翻译，同义词等等应用环境中，语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。  

RNN在理论上可以得到往前（往后）任意多个词的关系  

###　RNN

简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：  
![](http://upload-images.jianshu.io/upload_images/2256672-479f2a7488b91671.jpg)  

其特点简单来说就是，RNN的神经元的某些输出可以作为其输入再次传输到神经元中，因此可以利用之前（之后）的信息。  

如果我们把上面的图展开，循环神经网络也可以画成下面这个样子：  
![](http://upload-images.jianshu.io/upload_images/2256672-cf18bb1f06e750a4.jpg)   

用下面的公式来表示循环神经网络的计算方法：  
![](http://oyvmbp6uy.bkt.clouddn.com/20171229_2.png)

式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。

如果我们反复迭代，将得到：  
![](http://oyvmbp6uy.bkt.clouddn.com/20171229_3.png)

从上面可以看出，循环神经网络的输出值，是受前面历次输入值影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。

关于双向RNN以及BPTT算法的原理实现在后面的文章讲到（其实是没看完23333）

### LSTM

有时候，我们在处理当前任务的时候，只需要看一下比较近的一些信息。比如在一个语言模型中，我们要通过上文来预测一下个词可能会是什么，那么当我们看到“ the clouds are in the ?”时，不需要更多的信息，我们就能够自然而然的想到下一个词应该是“sky”。在这样的情况下，我们所要预测的内容和相关信息之间的间隔很小，这种情况下 RNNs 就能够利用过去的信息， 很容易的实现。

但是，有些情况是需要更多的上下文信息。比如我们要预测“I grew up in France … … I can speak ____”这个预测的词应该是 Franch，但是我们是要通过很长很长之前提到的信息，才能做出这个正确的预测的，理论上RNN可以做到，但是梯度弥散跟梯度爆炸的问题使得在实际操作中不能使用RNN。

LSTM能够解决这个问题。

长短期记忆网络（Long Short Term Memory networks） - 通常叫做 “LSTM” —— 是 RNN 中一个特殊的类型。由Hochreiter & Schmidhuber (1997)提出，广受欢迎，之后也得到了很多人们的改进调整。LSTM 被广泛地用于解决各类问题，并都取得了非常棒的效果。

简单来说，设计 LSTM 主要是为了避免前面提到的 长时期依赖 （long-term dependency ）的问题。它们的本质就是能够记住很长时期内的信息，而且非常轻松就能做到。

LSTM结构：
![](http://img.blog.csdn.net/20170228165331300?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

首先解释一下符号：
![](http://img.blog.csdn.net/20170228165554195?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

都比较容易理解。

LSTM单元上的那条直线代表了LSTM的状态state， 它会贯穿所有串联在一起的LSTM单元，其中只有少量的线性干预和改变，这些改变就是通过LSTM中的门（Gate）来控制，也就是单元中的下面部分，顾名思义门是用来控制信息是否通过的，下面详细讲解。

## 从左到右
### 遗忘门

首先是 LSTM 要决定让那些信息继续通过这个 cell，这是通过一个叫做“forget gate layer ”的sigmoid 神经层来实现的。它的输入是ht−1和xt，输出是一个数值都在 0，1 之间的向量（向量长度和 cell 的状态 Ct−1 一样），表示让 Ct−1 的各部分信息通过的比重。 0 表示“不让任何信息通过”， 1 表示“让所有信息通过”。

让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。
![](http://img.blog.csdn.net/20170301112027436?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 传入门

下一步是决定让多少新的信息加入到 cell 状态 中来。实现这个需要包括两个 步骤：首先，一个叫做“input gate layer ”的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容，C_t~ 。在下一步，我们把这两部分联合起来，对 cell 的状态进行一个更新。

![](http://img.blog.csdn.net/20170301115512234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

在我们的语言模型的例子中，我们想把新的主语性别信息添加到 cell 状态中，来替换掉老的状态信息。
有了上述的结构，我们就能够更新 cell 状态了， 即把Ct−1更新为 Ct。 从结构图中应该能一目了然， 首先我们把旧的状态 Ct−1和ft相乘， 把一些不想保留的信息忘掉。然后加上it∗C_t~。这部分信息就是我们要添加的新内容。

![](http://img.blog.csdn.net/20170301120227745?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 输出门

最后，我们需要来决定输出什么值了。这个输出主要是依赖于 cell 的状态Ct，但是又不仅仅依赖于 Ct，而是需要经过一个过滤的处理。首先，我们还是使用一个 sigmoid 层来（计算出）决定Ct中的哪部分信息会被输出。接着，我们把Ct通过一个 tanh 层（把数值都归到 -1 和 1 之间），然后把 tanh 层的输出和 sigmoid 层计算出来的权重相乘，这样就得到了最后输出的结果。

在语言模型例子中，假设我们的模型刚刚接触了一个代词，接下来可能要输出一个动词，这个输出可能就和代词的信息相关了。比如说，这个动词应该采用单数形式还是复数的形式，那么我们就得把刚学到的和代词相关的信息都加入到 cell 状态中来，才能够进行正确的预测。

![](http://img.blog.csdn.net/20170301121358058?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 后面的文章将介绍如何在Tensorflow中实现LSTM语言模型的建立
