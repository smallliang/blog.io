---
layout: post
title: 爬虫教程初级篇（一）——初识爬虫
date: 2018-09-09
categories: blog
tags: [爬虫]
author: gafei
---

# 什么是爬虫  
**爬虫**（又叫做Spider、网络蜘蛛等），是一种按一定规则自动获取互联网信息的程序或者脚本，用来收集数据信息作为后续处理用的原材料。  

# 爬虫的产生背景  

说起爬虫，不得不先提一下**爬虫**，上学期上了陈光老师的《网络搜索引擎原理》的课，一开始讲到爬虫，还以为是爬虫课，现在回过头来看老师讲的内容，爬虫只是搜索引擎的一部分。  

搜索引擎就在我们的身边，比如**Google**、**百度**、**Yahoo**等知名的搜索引擎，在输入一个字符串后，他们能在极短的时间内给出你需要的网站列表，它是帮助我们进入互联网世界的入口。  

网络爬虫是搜索引擎中的一部分，它从互联网中其他服务器中获取信息，不断向各种网站发出请求，保存各个网页信息以便分析。爬虫保证了搜索引擎的内容与质量。  

## 搜索引擎原理  

### 1.网络爬虫  

网络爬虫在互联网中从一个网站中的链接到其他网站爬取信息，不停的爬取，就像蜘蛛在蜘蛛网中爬来爬去一样。网络爬虫一般需要遵守一定的规则，每个网站都有自己制定的规则，哪些可以爬，哪些禁止爬，这些被写到了robots文件中。如：

<https://tieba.baidu.com/robots.txt>  
<https://www.1688.com/robots.txt>  

并且一些网站为了保证能够平稳运行，会有一些反爬虫机制，比如限制请求次数、封禁IP等，但是对于网络搜索引擎的爬虫，网站一般会配合爬取。

### 2.索引

爬取后的信息进行一些预处理之后被存入数据库，这么多数据如何进行快速的搜索你想要的网站数据呢，答案就是索引。索引能够帮助程序进行快速的查找，就跟新华字典里的偏旁索引、字母索引一样。  

介绍一下一种数据结构：**反转列表**  

搜索引擎中每个网站信息会保存成一个文档，文档中每个单词都有一个反转列表，记录这个单词在哪些文档中出现、出现次数、出现位置等许多信息，比如Google这个单词在22，55，765文档中出现，分别出现了3，6，86次，出现位置在…………，为了进行更快速的搜索，搜索引擎需要构造各式各样的反转列表以供使用。  

建立索引是一项巨大的工程，这里只是浅显的讲解了一些内容。  

### 3.搜索  

用户输入一串字符串，进行语义分词之后通过索引搜索，后台算法打分排序最后输出。  

# 爬虫的发展以及问题  

## 爬虫聚焦的问题  

目前在搜索引擎爬虫中的问题基本可以涵盖平常使用爬虫的问题：  

（1）Url的搜索策略  

（2）对数据的分析与过滤

（3）爬虫速度

（4）反爬虫

## 爬虫的发展趋势  

爬虫的趋势跟随着网页技术的发展，现在越来越多的网站使用Ajax等动态页面加载，以前的爬虫只能爬取静态页面的内容，是无法爬取到Ajax页面的信息的，我们需要解决：  

（1）JS在爬虫中的交互  

（2）动态DOM的内容抽取  

## 爬虫发展阶段（转）  

> 第一个阶段可以说是早期爬虫，斯坦福的几位同学完成的抓取，当时的互联网基本都是完全开放的，人类流量是主流；  

> 第二个阶段是分布式爬虫，但是爬虫面对新的问题是数据量越来越大，传统爬虫已经解决不了把数据都抓全的问题，需要更多的爬虫，于是调度问题就出现了；  

 > 第三阶段是暗网爬虫。此时面对新的问题是数据之间的link越来越少，比如淘宝，点评这类数据，彼此link很少，那么抓全这些数据就很难；还有一些数据是需要提交查询词才能获取，比如机票查询，那么需要寻找一些手段“发现”更多，更完整的不是明面上的数据。  

> 第四阶段智能爬虫，这主要是爬虫又开始面对新的问题：社交网络数据的抓取。  

> 社交网络对爬虫带来的新的挑战包括：  

> + 有一条账号护城河  
我们通常称UGC（User Generated Content）指用户原创内容。为web2.0，即数据从单向传达，到双向互动，人民群众可以与网站产生交互，因此产生了账号，每个人都通过账号来标识身份，提交数据，这样一来社交网络就可以通过封账号来提高数据抓取的难度，通过账号来发现非人类流量。之前没有账号只能通过cookie和ip。cookie又是易变，易挥发的，很难长期标识一个用户。  

> + 网络走向封闭  
新浪微博在2012年以前都是基本不封的，随便写一个程序怎么抓都不封，但是很快，越来越多的站点都开始防止竞争对手，防止爬虫来抓取，数据逐渐走向封闭，越来越多的人难以获得数据。甚至都出现了专业的爬虫公司，这在2010年以前是不可想象的。。  

> + 反爬手段，封杀手法千差万别  
写一个通用的框架抓取成百上千万的网站已经成为历史，或者说已经是一个技术相对成熟的工作，也就是已经有相对成熟的框架来”盗“成百上千的墓，但是极个别的墓则需要特殊手段了，目前市场上比较难以抓取的数据包括，微信公共账号，微博，facebook，ins，淘宝等等。具体原因各异，但基本无法用一个统一框架来完成，太特殊了。如果有一个通用的框架能解决我说的这几个网站的抓取，这一定是一个非常震撼的产品，如果有，一定要告诉我，那我公开出来，然后就改行了。  
当面对以上三个挑战的时候，就需要智能爬虫。智能爬虫是让爬虫的行为尽可能模仿人类行为，让反爬策略失效，只有”混在老百姓队伍里面，才是安全的“，因此这就需要琢磨浏览器了，很多人把爬虫写在了浏览器插件里面，把爬虫写在了手机里面，写在了路由器里面(春节抢票王)。再有一个传统的爬虫都是只有读操作的，没有写操作，这个很容易被判是爬虫，智能的爬虫需要有一些自动化交互的行为，这都是一些抵御反爬策略的方法。  

> 从商业价值上，是一个能够抽象千百万网站抓取框架的爬虫工程师值钱，还是一个能抓特定难抓网站的爬虫工程师值钱？  

> 能花钱来买，被市场认可的数据，都是那些特别难抓的，抓取成本异常高的数据。  

> 目前市场上主流的爬虫工程师，都是能够抓成百上千网站的数据，但如果想有价值，还是得有能力抓特别难抓的数据，才能估上好价钱。  

摘自[网络爬虫教程——了解爬虫](https://piaosanlang.gitbooks.io/spiders/01day/section1.2.html)
