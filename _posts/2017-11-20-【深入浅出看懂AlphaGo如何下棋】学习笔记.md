---
layout: post
title: 【深入浅出看懂AlphaGo如何下棋】学习笔记
date: 2017-11-20
categories: blog
tags: [Machine learning]
author: gafei
---

## 【深入浅出看懂AlphaGo如何下棋】by遥行   Go Further [文章链接](https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/)

### 问题
棋盘有`19*19 = 361`个点，每有个点三个状态，白棋1黑棋-1无棋子0，用$\vec{s}$来表示棋盘状态，$\vec{s}$是1*361的向量。  
然后记下一步的落子动作为$\vec{a}$，这也是一个361维向量。  
由此定义将问题转化为：
> 给定任意一个状态$\vec{s}$，寻找最优落子动作$\vec{a}$

### 解决方法
将棋盘看作一幅图像，用**深度卷积神经网络**（Deep Convolutional Neural Network）来解决。  
相关文章：如何理解CNN[An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)  

CNN可以对一幅图像进行处理，给定许多标注的样本进行训练，使得最后的神经网络获得具有分类效果的输出。 

同样，我们将一些围棋高手下过的棋谱拿过来放进CNN进行训练，那么结果就是：输入一个状态$\vec{s}$',输出落子$\vec{a}$'，表示一个接近人类下棋方式的落子，这个$\vec{a}$'是一个概率分布，比如在某处落子的概率为0.6，另一处为0.2……越接近1的点表示在这个位置越接近人类的风格，也可以等同于作为人类概率最大的落点。 

2015年，`Aja Huang`在ICLR的论文[Move Evaluation in Go Using Deep Convolutional Neural Networks](https://arxiv.org/pdf/1412.6564.pdf)中提出如何使用CNN解决围棋问题。  

他从围棋对战平台KGS上获得了人类选手的围棋对弈棋谱，对于每一个状态$\vec{s}$，都会有一个人类进行$\vec{a}$的落子，由此获得3000万个样本。然后他将$\vec{s}$看作`19*19`的二维图像，使用CNN进行训练，通过使用海量的数据，不断让计算机接近人类落子的位置。就可以得到一个模拟人类棋手下棋的神经网络。

我们记：$$\vec{a}=f(\vec{s})$$训练样本进行训练。然后他将
中的$f()$为$P_{human}=f(\vec{s})$，论文中称为**策略函数**，他表示：
> 在状态$\vec{s}$下，进行哪一个落子$\vec{a}$是最接近人类风格的

直观表示：  
![](https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/PolicyNetwork.png)

可以看到，红色的区域的值有60%，次大值位于右方，是35%（此图来自于AlphaGo论文）  

我们可以马上想到，这个围棋高手决定了这个训练结果的上限，也就是此时的结果受训练集的影响较大。

### 此时深度卷积神经网络的棋力
据`Aja Huang`表示，此时的神经网络的棋力大概达到业余6段的人类水平，如何突破自身的水平呢？当时的最强围棋电脑程序是`CrazyStone`，它是怎样解决围棋问题的。  

`Remi Colulum`在2006年对围棋AI做出的另一大重要突破：MCTS（蒙特卡洛搜索树）

[Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search](https://github.com/papers-we-love/papers-we-love/blob/5a54fa883a813e81b1e54bfed9669fc8961dedb4/artificial_intelligence/efficient-selectivity-and-backup-operators-in-monte-carlo-tree-search.pdf)

### MCTS蒙特卡洛搜索树
蒙特卡洛搜索树（Monte-Carlo Tree Search）是一种大智若愚的方法，它的基本思想是：

首先模拟一盘对决，使用的思路很简单，**随机**  

* 最初我们对棋盘一无所知，假设所有落子的方法分值都相等，设为1
* 之后，【随机】从361种方法中选一种走法$\vec{a}_{0}$，在这一步后，棋盘状态变为$\vec{s}_{1}$ 。之后假设对方也和自己一样，【随机】走了一步，此时棋盘状态变为$\vec{s}_{2}$。
* 重复以上步骤直到分出胜负，我们假设一个变量`r`，胜利记为1，失败则为0

虽然这么做看起来很蠢，但是最后还是有胜负，这至少说明了胜方的下棋方法比负方的稍微好那么一些些。。  
那么我们将胜方的落子方法$(\vec{s}_{0},\vec{a}_{0})$记录下来，新建一个公式：$$新分数=初始分数+r$$  

同理，可以把之后所有随机出来的落子方法$(\vec{s}_{i},\vec{a}_{i})$都应用上述公式，即都加1分。之后开始第二次模拟，这一次，我们对棋盘不是一无所知了，至少在$\vec{s}_{0}$状态时，我们知道$\vec{a}_{0}$落子方法比其他的360种好那么一些些，所以我们使用这个数据的方法是：在这次随机中，我们随机到$\vec{a}_{0}$落子方法的概率要比其他方法高一点。  

之后，我们不断重复以上步骤，这样，那些看起来不错（以最后的胜负来作为判断依据）的落子方案的分数就会越来越高，并且这些落子方案也是比较有前途的，会被更多的选择。

最后，当进行了10万盘棋后，在此状态选择那个分数最高的方案落子，此时，才真正下了这步棋。这种过程在论文里被称为**Rollout**

MCTS方法同人类原始探索的方法一样，都是不断的尝试，哪个方法好以后就多用这个方法，以此不断提升自己。

`Aja Huang`很快意识到这种方法的缺陷在哪里：随机的落子方式太过简单。人类对每种棋局状态都有更强的判断能力，是不是可以用$P_{human}=f(\vec{s})$来代替随机下棋的方式？  

`Aja Huang`改进了MCTS，每一步不使用随机，而是现根据$P_{human}=f(\vec{s})$来计算$\vec{a}$的概率分布，公式修改为：$$新分数=调整后的初始分+通过模拟的赢棋概率$$

但是那些人们常下的地方概率高，所以对初始分进行一下调整：$$调整后的初始分=P_{human}=f(\vec{s})/(被随机到的次数+1)$$

随着盘数的增加，虽然一开始可能会大概率选人类的落子方法，但是由于MCTS的随机性，会有别的下法，然后别的下法可能比人类的下法好，那么根据公式，人类下法概率会下降，这种随机出来的下法在下盘时的概率会增加。所以最终的结果可以比$P_{human}=f(\vec{s})$的下棋方法要好，这也就突破了原来的水平。  

### 强化学习——局面函数（Value Network）

看懂再写，，忘记了
