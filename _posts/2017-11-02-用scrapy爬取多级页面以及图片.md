---
layout: post
title: 用scrapy爬取多级页面以及图片
date: 2017-11-02
categories: blog
author: gafei
---

这次爬取的是天猫guitar（按销量）的信息以及图片  

爬虫程序爬取的目标通常不仅仅是文字资源，经常也会爬取图片资源。这就涉及如何高效下载图片的问题。这里高效下载指的是既能把图片完整下载到本地又不会对网站服务器造成压力。  

我们可以在 `pipeline` 中自己实现下载图片逻辑。但 `Scrapy` 提供了图片管道 `ImagesPipeline` ，方便我们操作下载图片。

### 1.确定爬取的数据
爬取的信息可以自己选择，这里我选择了`价格`，`商品名称`，`商品链接`，`店铺名`，`店铺链接`以及`图片链接`

![爬取目标](http://oyvmbp6uy.bkt.clouddn.com/20171102_1.png)
<center>
  <img src="http://oyvmbp6uy.bkt.clouddn.com/20171102_2.png"/>
</center>
<br>
其中的`价格`，`商品名称`，`商品链接`，`图片链接`可以在首页获得，而`店铺名`与`店铺链接`需要点击`商品链接`后的页面才能够获得，在确定完要爬取的数据之后，首先在创建一个新的scrapy项目
```python
scrapy startproject GuitarGoods
```
然后在项目中的```items.py```中这样编写：
```python
class GuitargoodsItem(scrapy.Item):
    GODDS_PRICE = scrapy.Field()
    GODDS_NAME = scrapy.Field()
    GOODS_URL = scrapy.Field()
    SHOP_NAME = scrapy.Field()
    SHOP_URL = scrapy.Field()
    # 图片链接
    PIC_URL = scrapy.Field()
```
### 2.解析网页
接下来对要爬取的网页进行解析  
首先打开浏览器，网页，按`F12`打开开发者工具，用这个工具找我们需要的信息块，如下图示首先找到一个大块，然后右键复制它的`Xpath`路径

![](http://oyvmbp6uy.bkt.clouddn.com/20171102_3.png)
<center>
  <img src="http://oyvmbp6uy.bkt.clouddn.com/20171102_4.jpg"/>
</center>
<br>
然后在scrapy项目的spiders文件夹中的爬虫文件里的`parse`函数进行编写：
```python
def parse(self, response):
    divs = response.xpath('//*[@id="J_ItemList"]/div')
```
复制的`Xpath`路径为`//*[@id="J_ItemList"]`这里我们多加了`/div`也就是取大块中的每一个小块，存储到列表中进行迭代  
这之后进行每一个小块中具体信息的提取，依旧是用原来的方法：
```python
for div in divs：
    item = GuitargoodsItem()
    price = div.xpath('div/p[1]/em/@title')[0].extract()
    goods_name = div.xpath('div/p[3]/a/@title')[0].extract()
    goods_url = div.xpath('div/p[3]/a/@href')[0].extract()
    goods_url = goods_url if "http:" in goods_url else ("http:" + goods_url)
    pic_url = div.xpath('div/div[1]/a/img/@src|''div/div[1]/a/img/@data-ks-lazyload')[0].extract()
```
这里需要注意的是，货物图片的路径有两种，可能是在`img`标签的`src`属性，也有可能在`data-ks-lazyload`属性，因此这里用或的方式写`Xpath`路径。  
在调试的过程不要急于求成，一个数据一个数据来提取，确保这个数据能够完全提取之后再进行下一个数据的提取  
在这里我们提取到了`价格`，`商品名称`，`商品链接`，`图片链接`，其他两个数据`店铺名`与`店铺链接`要点击商品链接的网页中获取。  
<center>
  <img src="http://oyvmbp6uy.bkt.clouddn.com/20171102_5.jpg"/>
</center>
<br>
接下来获取`店铺名`与`店铺链接`，将已经获取的4个内容都存储到item中，发起一个请求，请求的`url`是商品链接，元数据`meta`为item，回调函数`callback`是我们接下来要自己定义的函数`detail_parse()`
```python
item['GODDS_PRICE'] = price
item['GODDS_NAME'] = goods_name
item['GOODS_URL'] = goods_url
item['PIC_URL'] = ["http:" + pic_url]
yield Request(url=item['GOODS_URL'], meta={'item': item}, callback=self.detail_parse, dont_filter=True)
```
然后编写`detail_parse()`函数：
```python
def detail_parse(self, response):
    item = response.meta['item']
    shop_name = response.xpath('//*[@id="shopExtra"]/div[1]/a/strong/text()')[0].extract()
    shop_url = response.xpath('//*[@id="shopExtra"]/div[1]/a/@href')[0].extract()
    shop_url = shop_url if "http:" in shop_url else ("http:" + shop_url)
    item['SHOP_NAME'] = shop_name
    item['SHOP_URL'] = shop_url
    yield item
```
用相同的方法提取，这里需要讲一下，由于提取完这两项数据之后需要返回整个item，因此需要获得`parse()`函数中解析到的数据，如何传递数据呢，这里使用`meta`进行传输，在发送请求时将parse函数的item数据存入meta中，然后在`detail_parse`中取出来，就可以获得数据了，最后将所有数据返回到`pipeline`中进行处理。  
#### spider代码
```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.spiders import Request
from GuitarGoods.items import GuitargoodsItem

class GuitarGoodsSpider(scrapy.Spider):
    name = 'guitar_goods'
    allowed_domains = ['www.tmall.com']
    start_urls = ['https://list.tmall.com/search_product.htm?spm=a220m.1000858.1000724.4.45df383eyMyFYX&cat=50039583&q=%B5%E7%BC%AA%CB%FB&sort=d&style=g&from=.list.pc_1_searchbutton#J_Filter']
    count = 0 #处理个数


    def parse(self, response):

        divs = response.xpath('//*[@id="J_ItemList"]/div')

        for div in divs:
            item = GuitargoodsItem()
            price = div.xpath('div/p[1]/em/@title')[0].extract()
            goods_name = div.xpath('div/p[3]/a/@title')[0].extract()
            goods_url = div.xpath('div/p[3]/a/@href')[0].extract()
            goods_url = goods_url if "http:" in goods_url else ("http:" + goods_url)
            pic_url = div.xpath('div/div[1]/a/img/@src|''div/div[1]/a/img/@data-ks-lazyload')[0].extract()
            item['GODDS_PRICE'] = price
            item['GODDS_NAME'] = goods_name
            item['GOODS_URL'] = goods_url
            item['PIC_URL'] = ["http:" + pic_url]
            yield Request(url=item['GOODS_URL'], meta={'item': item}, callback=self.detail_parse, dont_filter=True)

    def detail_parse(self, response):
        item = response.meta['item']
        shop_name = response.xpath('//*[@id="shopExtra"]/div[1]/a/strong/text()')[0].extract()
        shop_url = response.xpath('//*[@id="shopExtra"]/div[1]/a/@href')[0].extract()
        shop_url = shop_url if "http:" in shop_url else ("http:" + shop_url)
        item['SHOP_NAME'] = shop_name
        item['SHOP_URL'] = shop_url
        yield item
```
### 3.存储数据以及图片
存储数据在`pipelines.py`文件中进行，这里使用mongoDB进行存储，scrapy会自动调用`process_item()`方法
```python
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import pymongo

class GuitargoodsPipeline(object):
    def __init__(self):
        self.connection = pymongo.MongoClient(host='127.0.0.1',
                                         port=27017)
        self.db = self.connection['test']  # 获得数据库的句柄
        self.coll = self.db['tm_guitar_top60']  # 获得collection的句柄

    def process_item(self, item, spider):
        del item['PIC_URL']
        postItem = dict(item)  # 把item转化成字典形式
        self.coll.insert(postItem)  # 向数据库插入一条记录
        return item  # 会在控制台输出原item数据，可以选择不写
```
这里删除不需要的`PIC_URL`字段，这个是用来下载图片用的所以不用存储，代码比较简单直接贴上来了  

最后在`settings.py`中启用pipeline以及自带的存储图片的`ImagesPipeline`
```python
ITEM_PIPELINES = {
   'GuitarGoods.pipelines.GuitargoodsPipeline': 300,
   'scrapy.pipelines.images.ImagesPipeline': 1,
}
IMAGES_URLS_FIELD = "PIC_URL"  #PIC_URL是在items.py中配置的网络爬取得图片地址
#配置保存本地的地址
project_dir = os.path.abspath(os.path.dirname(__file__))  #获取当前爬虫项目的绝对路径
IMAGES_STORE = os.path.join(project_dir, 'pic')  #组装新的图片路径
# 30 days of delay for images expiration
IMAGES_EXPIRES = 30
```
其中的`300`，`1`是优先级，选择优先下载图片还是优先存储数据，运行后会将图片下载到pic文件夹的full文件夹（自动创建）中  

![](http://oyvmbp6uy.bkt.clouddn.com/20171102_6.jpg)

## 写的不好的地方说一下。。
